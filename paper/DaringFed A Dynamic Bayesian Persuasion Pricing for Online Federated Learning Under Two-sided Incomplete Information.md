#### DaringFed: A Dynamic Bayesian Persuasion Pricing for Online Federated Learning Under Two-sided Incomplete Information

------

## 文章要解决的问题

**在线联邦学习 (OFL)** 场景下：

- **双边信息不完全**：
  - **服务器**不知道设备的真实计算/能耗成本；
  - **设备**不知道服务器的真实数据价值或模型需求。
- 现有激励机制大多假设信息对称或单边信息不完全，忽略了这种**双边不完全信息**情境。
- 导致的问题：设备可能隐瞒成本、服务器可能虚报需求，从而降低整体模型性能和系统效率。

本文要解决：**在双边信息不完全的情况下，如何设计动态激励机制，保证设备和服务器都愿意真实汇报并参与，从而提升 OFL 性能？**

------

## 作者提出的方案

 **DaringFed 框架**，其核心是：

1. **动态贝叶斯劝说机制 (Dynamic Bayesian Persuasion Pricing)**
   - 服务器通过“劝说”机制（persuasion）设计一个信号系统，诱导设备在不完全信息下仍作出对整体系统有利的选择。
   - 动态调整价格与信息传递，保证激励兼容性。
2. **双边激励机制**
   - 设备通过竞价或报价表达自己的隐私成本、能耗成本。
   - 服务器根据模型的训练需求和数据价值，决定支付与任务分配。
3. **在线学习特性**
   - 考虑了时间序列下的长期交互，动态更新价格与信号策略，而不是一次性的静态机制。

------

**DaringFed 机制**表示为一个二元组 (S,P)，即一个贝叶斯劝服信号规则 S 和一个动态定价规则 P。

​	**信号规则 (S)**：服务器发什么信号 σ，影响设备对通信资源 τ 的后验认知；

​	$ρ(σ∣τ)←ϕ(τ∣σ)/λ(τ)$

​	$ρ(σ∣τ)$：在给定通信资源 τ 的情况下，信号 σ 的分布（即“发什么信号”）。

​	$\phi(\tau|\sigma)$：在接收到信号 σ 后，对 τ 的 **后验分布**。

​	**定价规则 (P)**：根据信号和设备算力 θ 来决定奖励 γ。

​	$γ←ρ(σ∣τ)θ$

​	γ：给设备的奖励。$\rho(\sigma|\tau)$：由信号规则决定的概率权重。$\theta$：设备的计算资源能力。

设 μ为通信资源 τ 在接收到信号 σ 后的 **后验期望**。

为了确保服务器发出的信号既可信、又能诱导设备做出对平台有利的选择，同时保证统计一致性，DaringFed 要求满足三个条件：

1. Bayesian Consistency：

​	在接收到信号后，客户端对 τ 的后验期望必须等于 μ，**信号传递的信息必须和贝叶斯规则一致**。

2. Bayesian Plausibility：

​	所有信号下的期望后验均值 = 原始先验均值，即便服务器选择性发信号，**整体上平均后验不能和先验矛盾**。

3. Bayesian Benefit：

​	在信号机制下的期望参与概率 ≥ 没有信号机制时的期望参与概率，保证 **信号对平台有利**。



# 系统流程

**1. 初始化阶段**

遍历所有可能的客户端类型 θ∈Θ。此时客户端对通信资源 τ 的后验均值 μ 直接等于真实值 τ。

**2. 更新阶段**

当新设备到来时，根据历史信息（过去参与/不参与的记录、之前发出的奖励）更新分布 s(θ)。此时引入了 **奖励 γ** 和 **信号 ρ**，它们通过 Bayesian persuasion 影响客户端是否愿意参与训练。形成一个动态的、基于历史数据的 **后验分布 s(θ)**。

**3. 优化阶段**

在每个时间片中，遍历所有可能的奖励值 γ∈R，找到合适的 γ 和对应的信号分布 ρ，最小化服务器的总成本 $c_s$，保留下来的 (γ,ρ) 就是该时间片里要采用的奖励策略与信号规则。

**4. 迭代收敛**

奖励 γ 收敛到某个稳定值 γ+，或者设备的本地计算资源分布发生变化（此时需要重新计算新的最优 γ,ρ）。



 **总结**：
  **DaringFed 框架**，通过 **动态贝叶斯劝说定价机制** 来应对 **双边不完全信息** 问题，从而在联邦学习中实现更公平的激励分配，提升设备参与率和模型性能。

