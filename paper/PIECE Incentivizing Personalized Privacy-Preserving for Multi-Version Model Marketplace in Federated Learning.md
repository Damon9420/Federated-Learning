#### PIECE: Incentivizing Personalized Privacy-Preserving for Multi-Version Model Marketplace in Federated Learning

------

##  文章要解决的问题

在 **联邦学习 (FL)** 的模型市场（model marketplace）场景下：

- 不同用户对 **隐私保护** 的需求不同；
- 不同用户对 **模型版本** 的需求也不同；
- 现有研究主要关注单一模型或统一隐私要求，**缺乏对“个性化隐私偏好 + 多版本模型共存”的激励机制**。

 本文要解决：**如何在一个支持多版本模型的市场中，既满足不同用户的隐私需求，又能设计激励机制，保证参与者积极性和全局效率？**

------

##  作者提出的方案

 **PIECE 框架**，核心机制：

1. **个性化隐私保护 (Personalized Privacy-Preserving)**
   - 支持不同用户在不同隐私级别（如差分隐私参数 ε）下参与 FL。
   - 用户可选择隐私与收益的平衡点。
2. **多版本模型市场 (Multi-Version Model Marketplace)**
   - 不同用户可以获得不同版本的模型（如不同精度/不同成本），而不是一刀切的单一全局模型。
3. **激励机制设计 (Incentive Mechanism)**
   - 利用机制设计与博弈论方法，确保用户有动力真实汇报隐私偏好，并选择合适的模型版本。

------

##  流程步骤

### (1) 市场调研 

- Broker 对市场进行调研，收集买家的需求与估值分布：
- 得到预期的模型版本集合 M={M1,…,Mk}，并预测市场总收益。

------

### (2) 定价策略与价格发布 

- Broker 使用 **算法1** 确定最优版本价格 v∗，并计算总奖励池 $R_a$。

- Broker 在计算最优价格时，使用目标函数
  $\begin{array} {c}\min_{v}\lambda\sum_{i=1}^{k}\ell(v_i,P_i)-\sum_{j=1}^{k-1}[\log(v_j\varepsilon_{j+1}^m-v_{j+1}\varepsilon_j^m) \\ +\log(v_{j+1}-v_j)]-\log(v_1). \end{array}\quad(26)$

  约束模型逼近理想价格、防止套利并保证价格递增。

- 然后 Broker 公布：

  - 不同版本的模型价格；
  - 对应的激励预算。

------

### (3) 任务发布与奖励池设定 

- Broker 向数据拥有者发布联邦学习任务：
  - 包含任务目标、奖励池 $R_a$、支付规则 ρ、隐私规则 ϱ。
- 总奖励来自于模型交易的预期收益。
- **隐私规则 ϱ** 确保整个机制满足无套利和 DP 限制。

------

### (4) 数据拥有者选择隐私预算并训练 

- 每个数据拥有者根据：
  - 奖励分配规则 ρ，自己的隐私偏好与成本，系统给定的隐私限制 ϱ，
     选择最优的隐私预算 **PB ($\varepsilon_i^*$​)**。
- 在本地训练时：
  1. 数据拥有者计算并裁剪梯度；
  2. 按照 $\varepsilon_i^*$添加高斯噪声，保护隐私；
  3. 上传噪声化的更新到 Broker。
- Broker 聚合上传的结果，更新全局模型，并广播给参与者。

------

### (5) 模型生成与交易

- 训练完成后，Broker 进一步在全局模型基础上注入不同强度的噪声，生成多版本模型：
- Broker 将这些版本投放市场，供买家选择和交易。
- 最终形成 **模型交易收益**。

------

### (6) 支付与奖励分配 

- Broker 根据预设的 **支付规则 ρ** 分配奖励 $R_a$ 给数据拥有者。
- 确保支付满足个体理性和激励相容，即：
  - 数据拥有者参与不会吃亏；
  - 选择最优隐私预算是理性选择。

------

 **总结**：
 这篇文章提出了 **PIECE 框架**，在 **多版本模型市场** 中设计了一个支持 **个性化隐私保护** 的激励机制，同时防止套利行为的发出，利用博弈论建立效用函数与约束，确保隐私安全，提升用户参与度和模型性能。




